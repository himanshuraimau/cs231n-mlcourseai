{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Creating synthetic dataset...\n",
      "Vocabulary size: 52\n",
      "Initializing model...\n",
      "Starting training...\n",
      "Epoch: 01 | Time: 0.0m 43.45s\n",
      "\tTrain Loss: 0.149 | Train Acc: 93.83%\n",
      "\t Val. Loss: 0.005 |  Val. Acc: 99.99%\n",
      "\tBest model saved!\n",
      "Epoch: 02 | Time: 0.0m 43.78s\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.07%\n",
      "\t Val. Loss: 0.001 |  Val. Acc: 100.00%\n",
      "\tBest model saved!\n",
      "Epoch: 03 | Time: 0.0m 43.24s\n",
      "\tTrain Loss: 0.010 | Train Acc: 99.60%\n",
      "\t Val. Loss: 0.001 |  Val. Acc: 99.98%\n",
      "Epoch: 04 | Time: 0.0m 39.32s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.68%\n",
      "\t Val. Loss: 0.000 |  Val. Acc: 100.00%\n",
      "\tBest model saved!\n",
      "Epoch: 05 | Time: 0.0m 37.70s\n",
      "\tTrain Loss: 0.004 | Train Acc: 99.90%\n",
      "\t Val. Loss: 0.000 |  Val. Acc: 100.00%\n",
      "\tBest model saved!\n",
      "Test Loss: 0.000 | Test Acc: 100.00%\n",
      "\n",
      "Testing on sample texts:\n",
      "Text: 'this movie was good and amazing i enjoyed it'\n",
      "Sentiment: Positive (Score: 1.0000)\n",
      "\n",
      "Text: 'this film was terrible boring and a waste of time'\n",
      "Sentiment: Negative (Score: 0.0000)\n",
      "\n",
      "Text: 'the acting was brilliant perfect and wonderful'\n",
      "Sentiment: Positive (Score: 1.0000)\n",
      "\n",
      "Text: 'i hate this awful movie because it was horrible'\n",
      "Sentiment: Negative (Score: 0.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the RNN model\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # RNN layer (using GRU which is a more advanced RNN variant)\n",
    "        self.rnn = nn.GRU(embedding_dim, \n",
    "                          hidden_dim, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=True, \n",
    "                          dropout=dropout if n_layers > 1 else 0,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # Embed the text\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # Pack the sequence (for efficiency with padded sequences)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), \n",
    "                                                          batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through the RNN\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Get the final hidden state from both directions\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize (simple space-based tokenization)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Function to create batches\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    text_lengths = torch.tensor([len(text) for text in texts])\n",
    "    \n",
    "    # Sort by length in descending order\n",
    "    sorted_indices = torch.argsort(text_lengths, descending=True)\n",
    "    sorted_texts = [texts[i] for i in sorted_indices]\n",
    "    sorted_lengths = text_lengths[sorted_indices]\n",
    "    sorted_labels = [labels[i] for i in sorted_indices]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_texts = nn.utils.rnn.pad_sequence([torch.tensor(text) for text in sorted_texts], \n",
    "                                            batch_first=True, \n",
    "                                            padding_value=PAD_IDX)\n",
    "    \n",
    "    return padded_texts, torch.tensor(sorted_labels, dtype=torch.float), sorted_lengths\n",
    "\n",
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        texts, labels, text_lengths = batch\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (predictions == labels).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            texts, labels, text_lengths = batch\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(texts, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            predictions = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (predictions == labels).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Constants\n",
    "    MAX_VOCAB_SIZE = 10000\n",
    "    MAX_SEQ_LENGTH = 100\n",
    "    BATCH_SIZE = 64\n",
    "    EMBEDDING_DIM = 100\n",
    "    HIDDEN_DIM = 256\n",
    "    OUTPUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.5\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    LR = 0.001\n",
    "    N_EPOCHS = 5\n",
    "    \n",
    "    # Create a simple synthetic dataset\n",
    "    print(\"Creating synthetic dataset...\")\n",
    "    \n",
    "    # Positive words for our synthetic data\n",
    "    positive_words = ['good', 'great', 'excellent', 'fantastic', 'wonderful', 'amazing', 'love', \n",
    "                      'best', 'happy', 'enjoyed', 'favorite', 'recommend', 'brilliant', 'perfect']\n",
    "    \n",
    "    # Negative words for our synthetic data\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disappointing', 'waste', 'hate', \n",
    "                      'worst', 'boring', 'poor', 'dislike', 'avoid', 'failed', 'annoying']\n",
    "    \n",
    "    # Common words\n",
    "    common_words = ['the', 'a', 'an', 'and', 'but', 'or', 'because', 'as', 'of', 'for', 'in', \n",
    "                   'to', 'with', 'on', 'at', 'by', 'this', 'that', 'these', 'those', 'is', 'was']\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = {}\n",
    "    vocab['<pad>'] = PAD_IDX\n",
    "    vocab['<unk>'] = UNK_IDX\n",
    "    \n",
    "    for i, word in enumerate(common_words + positive_words + negative_words):\n",
    "        vocab[word] = i + 2\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Function to generate a synthetic review\n",
    "    def generate_review(is_positive, min_length=5, max_length=50):\n",
    "        length = random.randint(min_length, max_length)\n",
    "        sentiment_words = positive_words if is_positive else negative_words\n",
    "        \n",
    "        # Generate more sentiment words for stronger signal\n",
    "        sentiment_word_count = random.randint(max(1, length // 8), max(3, length // 4))\n",
    "        common_word_count = length - sentiment_word_count\n",
    "        \n",
    "        # Select random words\n",
    "        selected_sentiment = [random.choice(sentiment_words) for _ in range(sentiment_word_count)]\n",
    "        selected_common = [random.choice(common_words) for _ in range(common_word_count)]\n",
    "        \n",
    "        # Mix them together\n",
    "        review = selected_common + selected_sentiment\n",
    "        random.shuffle(review)\n",
    "        \n",
    "        # Convert to indices\n",
    "        return [vocab.get(word, UNK_IDX) for word in review]\n",
    "    \n",
    "    # Generate dataset\n",
    "    num_samples = 10000  # 5000 each for train and test\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for i in range(num_samples // 2):\n",
    "        # Positive samples\n",
    "        train_texts.append(generate_review(True))\n",
    "        train_labels.append(1)\n",
    "        test_texts.append(generate_review(True))\n",
    "        test_labels.append(1)\n",
    "        \n",
    "        # Negative samples\n",
    "        train_texts.append(generate_review(False))\n",
    "        train_labels.append(0)\n",
    "        test_texts.append(generate_review(False))\n",
    "        test_labels.append(0)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_texts, train_labels)\n",
    "    test_dataset = TextDataset(test_texts, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = SentimentRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT, PAD_IDX).to(device)\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best-model.pt')\n",
    "            print(\"\\tBest model saved!\")\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    model.load_state_dict(torch.load('best-model.pt'))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Function to predict sentiment of a new text\n",
    "    def predict_sentiment(model, text):\n",
    "        model.eval()\n",
    "        tokens = preprocess_text(text)\n",
    "        indices = [vocab.get(token, UNK_IDX) for token in tokens]\n",
    "        tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "        length_tensor = torch.tensor([len(indices)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "        \n",
    "        return prediction.item()\n",
    "    \n",
    "    # Example usage\n",
    "    sample_texts = [\n",
    "        \"this movie was good and amazing i enjoyed it\",\n",
    "        \"this film was terrible boring and a waste of time\",\n",
    "        \"the acting was brilliant perfect and wonderful\",\n",
    "        \"i hate this awful movie because it was horrible\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting on sample texts:\")\n",
    "    for text in sample_texts:\n",
    "        sentiment = predict_sentiment(model, text)\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Sentiment: {'Positive' if sentiment > 0.5 else 'Negative'} (Score: {sentiment:.4f})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
